{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enforce Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 44\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(42)  # if using multi-GPU\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained GPT-2 Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (GPT-2)\n",
    "base_gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add padding token for the tokenizer if not present\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "base_gpt2_model.config.pad_token_id = base_gpt2_model.config.eos_token_id\n",
    "\n",
    "# Add special tokens to the tokenizer\n",
    "special_tokens = {\n",
    "    'bos_token': '<|bos|>',\n",
    "    'eos_token': '<|eos|>',\n",
    "    'pad_token': '<|pad|>',\n",
    "    'sep_token': '<|sep|>'\n",
    "}\n",
    "gpt2_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Initialize model with resized token embeddings\n",
    "base_gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "\n",
    "# Set the pad_token_id in the model configuration\n",
    "base_gpt2_model.config.pad_token_id = gpt2_tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_e2e_for_gpt2(tokenizer):\n",
    "    dataset = load_dataset(\"e2e_nlg\")\n",
    "    # print(dataset[\"test\"][\"meaning_representation\"])\n",
    "\n",
    "    # # save test set for metric computation with official script\n",
    "    # testSet = dataset[\"test\"]\n",
    "    # df = testSet.to_pandas()\n",
    "    # df.to_csv(\"e2e_nlg_test_set.csv\", index=False)\n",
    "\n",
    "    # Add special tokens\n",
    "    special_tokens = {\n",
    "        'bos_token': '<|bos|>',\n",
    "        'eos_token': '<|eos|>',\n",
    "        'pad_token': '<|pad|>',\n",
    "        'sep_token': '<|sep|>'\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    def preprocess_train(examples):\n",
    "        # Format: <|bos|> meaning_representation <|sep|> target <|eos|>\n",
    "        texts = [\n",
    "            f\"{tokenizer.bos_token} {mr} {tokenizer.sep_token} {ref} {tokenizer.eos_token}\"\n",
    "            for mr, ref in zip(examples['meaning_representation'], examples['human_reference'])\n",
    "        ]\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "        )\n",
    "        \n",
    "        # Create labels (same as input_ids initially)\n",
    "        labels = encodings['input_ids']\n",
    "    \n",
    "        # Mask out MR tokens in labels\n",
    "        for i, (mr, ref) in enumerate(zip(examples['meaning_representation'], examples['human_reference'])):\n",
    "            # Tokenize MR and find its length\n",
    "            mr_tokens = tokenizer(f\"{tokenizer.bos_token} {mr} {tokenizer.sep_token}\", add_special_tokens=False).input_ids\n",
    "        \n",
    "            # Mask MR tokens with -100\n",
    "            num_mr_tokens = len(mr_tokens)\n",
    "            labels[i][:num_mr_tokens] = [-100] * num_mr_tokens\n",
    "    \n",
    "        # Add labels to the encodings\n",
    "        encodings['labels'] = labels\n",
    "        return encodings\n",
    "\n",
    "        # # Create labels (same as input_ids)\n",
    "        # encodings['labels'] = encodings['input_ids']\n",
    "        # return encodings\n",
    "    \n",
    "\n",
    "    def preprocess_test(examples):\n",
    "        mr_texts = [\n",
    "            f\"{tokenizer.bos_token} {mr} {tokenizer.sep_token} \"\n",
    "            for mr in examples['meaning_representation']\n",
    "        ]\n",
    "\n",
    "        mr_encodings = tokenizer(\n",
    "            mr_texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=64,\n",
    "        )\n",
    "    \n",
    "        ref_texts = [\n",
    "            f\"{ref} {tokenizer.eos_token}\"\n",
    "            for ref in examples['human_reference']\n",
    "        ]\n",
    "\n",
    "        ref_encodings = tokenizer(\n",
    "            ref_texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=64,\n",
    "        )\n",
    "\n",
    "        mr_encodings['labels'] = ref_encodings['input_ids']\n",
    "        return mr_encodings\n",
    "\n",
    "\n",
    "    # Process all splits\n",
    "    tokenized_train = dataset.map(\n",
    "        preprocess_train,\n",
    "        batched=True,\n",
    "        remove_columns=dataset['train'].column_names\n",
    "    )\n",
    "\n",
    "    tokenized_test = dataset.map(\n",
    "        preprocess_test,\n",
    "        batched=True,\n",
    "    )\n",
    "\n",
    "    # Set format to PyTorch tensors\n",
    "    tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    return tokenized_train, tokenized_test\n",
    "#=====================================================\n",
    "\n",
    "tokenized_train, tokenized_test = prep_e2e_for_gpt2(gpt2_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from nltk.translate.nist_score import sentence_nist\n",
    "\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "cider = Cider()\n",
    "meteor = Meteor()\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions = pred[\"predictions\"]\n",
    "    labels = pred[\"label_ids\"]\n",
    "    \n",
    "    print(predictions[0])\n",
    "    # print(labels[0])\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = predictions #gpt2_tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = labels #gpt2_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # decoded_labels = [[label] for label in decoded_labels]  # Format for some metrics\n",
    "\n",
    "    # Compute BLEU\n",
    "    bleu_score = bleu.compute(predictions=decoded_preds, references=decoded_labels)[\"bleu\"]\n",
    "    \n",
    "    # Compute ROUGE-L\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels, rouge_types=[\"rougeL\"])\n",
    "    rougeL_score = rouge_result[\"rougeL\"]\n",
    "    \n",
    "    # Compute CIDEr\n",
    "    # Format the predictions and references into dictionaries with index as key\n",
    "    # CIDEr expects the input format to be a dictionary with example ID as the key\n",
    "    refs = {i: [label] for i, label in enumerate(decoded_labels)}\n",
    "    preds = {i: [pred] for i, pred in enumerate(decoded_preds)}\n",
    "    cider_score, _ = cider.compute_score(refs, preds)\n",
    "    \n",
    "    # Compute METEOR\n",
    "    meteor_score, _ = meteor.compute_score(preds, refs)\n",
    "    \n",
    "    # Compute NIST\n",
    "    nist_scores = []\n",
    "    decoded_labels = [label.split() for label in decoded_labels]\n",
    "    # print(f\"decoded Labels: {decoded_labels}\")\n",
    "    for candidate in decoded_preds:\n",
    "        nist_score = sentence_nist(decoded_labels, candidate.split())  # You can adjust n as needed\n",
    "        nist_scores.append(nist_score)\n",
    "\n",
    "    avg_nist_score = sum(nist_scores) / len(nist_scores) if nist_scores else 0  # Average NIST score\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"nist\": avg_nist_score,\n",
    "        \"meteor\": meteor_score,\n",
    "        \"rougeL\": rougeL_score,\n",
    "        \"cider\": cider_score,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom Eval/Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def evaluate_with_custom_generation(trainer, out_filename, beam_size, length_penalty, no_repeat_ngram_size, batch_size=8):\n",
    "    model = trainer.model\n",
    "    tokenizer = gpt2_tokenizer\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # DataLoader for evaluation dataset\n",
    "    eval_dataloader = DataLoader(\n",
    "        trainer.eval_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    eval_predictions = []\n",
    "    eval_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "\n",
    "            # print(f\"batch: {tokenizer.batch_decode(input_ids, skip_special_tokens=True)}\")\n",
    "            # print(f\"input ids: {input_ids}\")\n",
    "            \n",
    "            # Generate predictions with beam search\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=beam_size,\n",
    "                length_penalty=length_penalty,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                max_length=input_ids.size(1) + 40,\n",
    "                eos_token_id=model.config.eos_token_id,  # Ensure eos_token_id is set\n",
    "                early_stopping=True,\n",
    "            )\n",
    "\n",
    "            # print(model.config.eos_token_id)\n",
    "            # Remove the input_ids from the generated outputs\n",
    "            # Slice the generated output to remove input part (this assumes `outputs` includes the input tokens)\n",
    "            generated_ids = outputs[:, input_ids.size(1):]  # Skip the input length\n",
    "\n",
    "            # Decode predictions\n",
    "            decoded_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            eval_predictions.extend(decoded_outputs)\n",
    "\n",
    "            # Decode labels for metric comparison\n",
    "            decoded_labels = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "            eval_labels.extend(decoded_labels)\n",
    "            break  #=====================================================================\n",
    "\n",
    "    # Prepare predictions and labels for metric computation\n",
    "    pred = {\n",
    "        'predictions': eval_predictions,\n",
    "        'label_ids': eval_labels\n",
    "    }\n",
    "\n",
    "    # save preds to file\n",
    "    with open(out_filename, 'w') as f:\n",
    "        f.writelines(item + \"\\n\" for item in eval_predictions)\n",
    "\n",
    "    # Compute custom evaluation metrics\n",
    "    custom_eval_metrics = trainer.compute_metrics(pred)\n",
    "    print(\"Evaluation Metrics:\", custom_eval_metrics)\n",
    "\n",
    "    return custom_eval_metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LoRA Config and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 14\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# From the original implementation in https://arxiv.org/abs/2106.09685\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Note that we are using vanilla GPT-2 (i.e. GPT-2 Small), rather than the original GPT-2 Medium\u001b[39;00m\n\u001b[1;32m      5\u001b[0m base_lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      6\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      7\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     init_lora_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_gpt2_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_lora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Print trainable parameters\u001b[39;00m\n\u001b[1;32m     16\u001b[0m trainable_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m base_model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/peft/mapping.py:193\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    192\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/peft/peft_model.py:1609\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1607\u001b[0m     \u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1608\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1609\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/peft/peft_model.py:171\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    169\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_adapter_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/peft/tuners/lora/model.py:141\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name, low_cpu_mem_usage: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:184\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:496\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    494\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[0;32m--> 496\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m tied_target_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tied_target_modules(model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tied_target_modules:\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/peft/tuners/lora/model.py:216\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madalora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaLoraLayer\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, LoraLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, AdaLoraLayer):\n\u001b[0;32m--> 216\u001b[0m     \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_lora_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_lora_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_rslora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_rslora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_dora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_dora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_module(lora_config, adapter_name, target, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/peft/tuners/lora/layer.py:139\u001b[0m, in \u001b[0;36mLoraLayer.update_layer\u001b[0;34m(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora, use_dora)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_lora_parameters(adapter_name, init_lora_weights)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# call this before dora_init\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_adapter_to_device_of_base_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_dora:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdora_init(adapter_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:821\u001b[0m, in \u001b[0;36mBaseTunerLayer._move_adapter_to_device_of_base_layer\u001b[0;34m(self, adapter_name, device)\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point \u001b[38;5;129;01mor\u001b[39;00m weight\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_complex:\n\u001b[0;32m--> 821\u001b[0m     adapter_layer[adapter_name] \u001b[38;5;241m=\u001b[39m \u001b[43madapter_layer\u001b[49m\u001b[43m[\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    823\u001b[0m     adapter_layer[adapter_name] \u001b[38;5;241m=\u001b[39m adapter_layer[adapter_name]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# From the original implementation in https://arxiv.org/abs/2106.09685\n",
    "# Note that we are using vanilla GPT-2 (i.e. GPT-2 Small), rather than the original GPT-2 Medium\n",
    "base_lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\"],  # only W_q and W_v are used in the benchmark\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # TaskType.LM,\n",
    "    init_lora_weights = True,\n",
    ")\n",
    "base_model = get_peft_model(base_gpt2_model, base_lora_config)\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "# all batches originally 8, but made smaller bc ran out of memory\n",
    "base_training_args = TrainingArguments(\n",
    "    output_dir=\"./base_results\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    label_smoothing_factor=0.1,\n",
    "    logging_dir='./base_logs',\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "base_trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=base_training_args,\n",
    "    train_dataset=tokenized_train['train'],\n",
    "    eval_dataset=tokenized_test['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "\n",
    "base_trainer.train()\n",
    "base_trainer.save_model(\"./base_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "BASE METRICS:\n",
      "batch: [' name[Blue Spice], eatType[coffee shop], area[city centre]  ', ' name[Blue Spice], eatType[coffee shop], area[city centre]  ', ' name[Blue Spice], eatType[coffee shop], area[riverside]  ', ' name[Blue Spice], eatType[coffee shop], area[riverside]  ', ' name[Blue Spice], eatType[coffee shop], customer rating[5 out of 5], near[Crowne Plaza Hotel]  ', ' name[Blue Spice], eatType[coffee shop], customer rating[5 out of 5], near[Crowne Plaza Hotel]  ', ' name[Blue Spice], eatType[coffee shop], customer rating[5 out of 5], near[Crowne Plaza Hotel]  ', ' name[Blue Spice], eatType[coffee shop], customer rating[average], near[Burger King]  ']\n",
      "                   � ................  \n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBASE METRICS:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m base_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_with_custom_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase-output.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 64\u001b[0m, in \u001b[0;36mevaluate_with_custom_generation\u001b[0;34m(trainer, out_filename, beam_size, length_penalty, no_repeat_ngram_size, batch_size)\u001b[0m\n\u001b[1;32m     61\u001b[0m     f\u001b[38;5;241m.\u001b[39mwritelines(item \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m eval_predictions)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Compute custom evaluation metrics\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m custom_eval_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m, custom_eval_metrics)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m custom_eval_metrics\n",
      "Cell \u001b[0;32mIn[9], line 46\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print(f\"decoded Labels: {decoded_labels}\")\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m decoded_preds:\n\u001b[0;32m---> 46\u001b[0m     nist_score \u001b[38;5;241m=\u001b[39m \u001b[43msentence_nist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# You can adjust n as needed\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     nist_scores\u001b[38;5;241m.\u001b[39mappend(nist_score)\n\u001b[1;32m     49\u001b[0m avg_nist_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(nist_scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(nist_scores) \u001b[38;5;28;01mif\u001b[39;00m nist_scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Average NIST score\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/nltk/translate/nist_score.py:70\u001b[0m, in \u001b[0;36msentence_nist\u001b[0;34m(references, hypothesis, n)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_nist\u001b[39m(references, hypothesis, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Calculate NIST score from\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    George Doddington. 2002. \"Automatic evaluation of machine translation quality\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    :type n: int\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorpus_nist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/nltk/translate/nist_score.py:165\u001b[0m, in \u001b[0;36mcorpus_nist\u001b[0;34m(list_of_references, hypotheses, n)\u001b[0m\n\u001b[1;32m    162\u001b[0m nist_precision \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nist_precision_numerator_per_ngram:\n\u001b[1;32m    164\u001b[0m     precision \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 165\u001b[0m         \u001b[43mnist_precision_numerator_per_ngram\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnist_precision_denominator_per_ngram\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m     nist_precision \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m precision\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Eqn 3 in Doddington(2002)\u001b[39;00m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# reload model and trainer\n",
    "reloaded_base_model = PeftModel.from_pretrained(base_gpt2_model, \"./base_model\")\n",
    "\n",
    "# reloaded_base_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "base_trainer = Trainer(\n",
    "    model=reloaded_base_model,\n",
    "    args=base_training_args,\n",
    "    train_dataset=tokenized_train['train'],\n",
    "    eval_dataset=tokenized_test['test'],  #tokenized_train['validation'],  #['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "beam_size = 10\n",
    "length_penalty = 0.9\n",
    "no_repeat_ngram_size = 4\n",
    "\n",
    "print('='*20)\n",
    "print(\"BASE METRICS:\")\n",
    "base_metrics = evaluate_with_custom_generation(base_trainer, out_filename=\"base-output.txt\", beam_size=beam_size, length_penalty=length_penalty, no_repeat_ngram_size=no_repeat_ngram_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better LoRA Config and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 3204432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "better_gpt2_model = deepcopy(base_gpt2_model)\n",
    "\n",
    "# My implementation to improve upon baseline\n",
    "better_lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\", \"wte\", \"wpe\"],  # only W_q and W_v are used in the benchmark\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # TaskType.LM,\n",
    "    init_lora_weights = \"gaussian\",\n",
    ")\n",
    "better_model = get_peft_model(better_gpt2_model, better_lora_config)\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in better_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "better_training_args = TrainingArguments(\n",
    "    output_dir=\"./better_results\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    label_smoothing_factor=0.1,\n",
    "    logging_dir='./better_logs',\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# better_trainer = Trainer(\n",
    "#     model=better_model,\n",
    "#     args=better_training_args,\n",
    "#     train_dataset=tokenized_train['train'],\n",
    "#     eval_dataset=tokenized_test['test'],\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    "# )\n",
    "\n",
    "\n",
    "# better_trainer.train()\n",
    "# better_trainer.save_model(\"./better_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Better Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "BETTER METRICS:\n",
      "Evaluation Metrics: {'bleu': 0.20487219861156433, 'nist': 2.5084701959526274, 'meteor': 0.23775311842805094, 'rougeL': 0.46319725238068893, 'cider': 0.12804611983750253}\n"
     ]
    }
   ],
   "source": [
    "# reload model and trainer\n",
    "reloaded_better_model = PeftModel.from_pretrained(better_gpt2_model, \"./better_model\")\n",
    "\n",
    "# reloaded_base_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "better_trainer = Trainer(\n",
    "    model=reloaded_better_model,\n",
    "    args=better_training_args,\n",
    "    train_dataset=tokenized_train['train'],\n",
    "    eval_dataset=tokenized_test['test'],  #['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "beam_size = 10\n",
    "length_penalty = 0.9\n",
    "no_repeat_ngram_size = 4\n",
    "\n",
    "print('='*20)\n",
    "print(\"BETTER METRICS:\")\n",
    "better_metrics = evaluate_with_custom_generation(better_trainer, out_filename=\"better-output.txt\", beam_size=beam_size, length_penalty=length_penalty, no_repeat_ngram_size=no_repeat_ngram_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "567",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
