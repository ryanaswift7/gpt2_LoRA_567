{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enforce Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 44\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(42)  # if using multi-GPU\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained GPT-2 Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (GPT-2)\n",
    "base_gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add padding token for the tokenizer if not present\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "base_gpt2_model.config.pad_token_id = base_gpt2_model.config.eos_token_id\n",
    "\n",
    "# Add special tokens to the tokenizer\n",
    "special_tokens = {\n",
    "    'bos_token': '<|bos|>',\n",
    "    'eos_token': '<|eos|>',\n",
    "    'pad_token': '<|pad|>',\n",
    "    'sep_token': '<|sep|>'\n",
    "}\n",
    "gpt2_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Initialize model with resized token embeddings\n",
    "base_gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "\n",
    "# Set the pad_token_id in the model configuration\n",
    "base_gpt2_model.config.pad_token_id = gpt2_tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_e2e_for_gpt2(tokenizer):\n",
    "    dataset = load_dataset(\"e2e_nlg\")\n",
    "\n",
    "    # Add special tokens\n",
    "    special_tokens = {\n",
    "        'bos_token': '<|bos|>',\n",
    "        'eos_token': '<|eos|>',\n",
    "        'pad_token': '<|pad|>',\n",
    "        'sep_token': '<|sep|>'\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Format: <|bos|> meaning_representation <|sep|> target <|eos|>\n",
    "        texts = [\n",
    "            f\"{tokenizer.bos_token} {mr} {tokenizer.sep_token} {ref} {tokenizer.eos_token}\"\n",
    "            for mr, ref in zip(examples['meaning_representation'], examples['human_reference'])\n",
    "        ]\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "        )\n",
    "        \n",
    "        # Create labels (same as input_ids)\n",
    "        encodings['labels'] = encodings['input_ids']\n",
    "        return encodings\n",
    "    \n",
    "    # Process all splits\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset['train'].column_names\n",
    "    )\n",
    "    \n",
    "    # Set format to PyTorch tensors\n",
    "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    return tokenized_dataset\n",
    "#=====================================================\n",
    "\n",
    "tokenized_dataset = prep_e2e_for_gpt2(gpt2_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from nltk.translate.nist_score import sentence_nist\n",
    "\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "cider = Cider()\n",
    "meteor = Meteor()\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions = pred[\"predictions\"]\n",
    "    labels = pred[\"label_ids\"]\n",
    "    \n",
    "    print(predictions[0])\n",
    "    print(labels[0])\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = predictions #gpt2_tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = labels #gpt2_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # decoded_labels = [[label] for label in decoded_labels]  # Format for some metrics\n",
    "\n",
    "    # Compute BLEU\n",
    "    bleu_score = bleu.compute(predictions=decoded_preds, references=decoded_labels)[\"bleu\"]\n",
    "    \n",
    "    # Compute ROUGE-L\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels, rouge_types=[\"rougeL\"])\n",
    "    rougeL_score = rouge_result[\"rougeL\"]\n",
    "    \n",
    "    # Compute CIDEr\n",
    "    # Format the predictions and references into dictionaries with index as key\n",
    "    # CIDEr expects the input format to be a dictionary with example ID as the key\n",
    "    refs = {i: [label] for i, label in enumerate(decoded_labels)}\n",
    "    preds = {i: [pred] for i, pred in enumerate(decoded_preds)}\n",
    "    cider_score, _ = cider.compute_score(refs, preds)\n",
    "    \n",
    "    # Compute METEOR\n",
    "    meteor_score, _ = meteor.compute_score(preds, refs)\n",
    "    \n",
    "    # Compute NIST\n",
    "    nist_scores = []\n",
    "    decoded_labels = [label.split() for label in decoded_labels]\n",
    "    print(f\"decoded Labels: {decoded_labels}\")\n",
    "    for candidate in decoded_preds:\n",
    "        nist_score = sentence_nist(decoded_labels, candidate.split())  # You can adjust n as needed\n",
    "        nist_scores.append(nist_score)\n",
    "\n",
    "    avg_nist_score = sum(nist_scores) / len(nist_scores) if nist_scores else 0  # Average NIST score\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"nist\": avg_nist_score,\n",
    "        \"meteor\": meteor_score,\n",
    "        \"rougeL\": rougeL_score,\n",
    "        \"cider\": cider_score,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom Eval/Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_with_custom_generation(trainer, beam_size, length_penalty, no_repeat_ngram_size):\n",
    "#     model = trainer.model\n",
    "#     tokenizer = trainer.tokenizer\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     eval_predictions = []\n",
    "    \n",
    "#     with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "#         for example in trainer.eval_dataset:\n",
    "#             input_ids = example['input_ids']\n",
    "#             # Generate predictions with the specified beam size\n",
    "#             outputs = model.generate(input_ids=input_ids, num_beams=beam_size, length_penalty=length_penalty, no_repeat_ngram_size=no_repeat_ngram_size)\n",
    "#             decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#             eval_predictions.append(decoded_output)\n",
    "#     # Compute metrics on the evaluation predictions\n",
    "#     pred = {\n",
    "#         'predictions': tokenizer.batch_encode_plus(eval_predictions, return_tensors='pt', padding=True)['input_ids'],\n",
    "#         'label_ids': tokenizer.batch_encode_plus(trainer.eval_dataset['validation']['target'], return_tensors='pt', padding=True)['input_ids']\n",
    "#     }\n",
    "#     custom_eval_metrics = compute_metrics(pred)\n",
    "#     print(\"Evaluation Metrics:\", custom_eval_metrics)\n",
    "#     return custom_eval_metrics\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def evaluate_with_custom_generation(trainer, beam_size, length_penalty, no_repeat_ngram_size, batch_size=8):\n",
    "    model = trainer.model\n",
    "    tokenizer = gpt2_tokenizer\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # DataLoader for evaluation dataset\n",
    "    eval_dataloader = DataLoader(\n",
    "        trainer.eval_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    eval_predictions = []\n",
    "    eval_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "            \n",
    "            # Generate predictions with beam search\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=beam_size,\n",
    "                length_penalty=length_penalty,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                max_length=150,\n",
    "            )\n",
    "\n",
    "            # Decode predictions\n",
    "            decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            eval_predictions.extend(decoded_outputs)\n",
    "\n",
    "            # Decode labels for metric comparison\n",
    "            decoded_labels = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "            eval_labels.extend(decoded_labels)\n",
    "            break  #=====================================================================\n",
    "\n",
    "    # Prepare predictions and labels for metric computation\n",
    "    pred = {\n",
    "        'predictions': eval_predictions,\n",
    "        'label_ids': eval_labels\n",
    "    }\n",
    "\n",
    "    # Compute custom evaluation metrics\n",
    "    custom_eval_metrics = trainer.compute_metrics(pred)\n",
    "    print(\"Evaluation Metrics:\", custom_eval_metrics)\n",
    "\n",
    "    return custom_eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LoRA Config and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 147456\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# From the original implementation in https://arxiv.org/abs/2106.09685\n",
    "# Note that we are using vanilla GPT-2 (i.e. GPT-2 Small), rather than the original GPT-2 Medium\n",
    "base_lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\"],  # only W_q and W_v are used in the benchmark\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # TaskType.LM,\n",
    "    init_lora_weights = True,\n",
    ")\n",
    "base_model = get_peft_model(base_gpt2_model, base_lora_config)\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "# all batches originally 8, but made smaller bc ran out of memory\n",
    "base_training_args = TrainingArguments(\n",
    "    output_dir=\"./base_results\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    label_smoothing_factor=0.1,\n",
    "    logging_dir='./base_logs',\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# base_trainer = Trainer(\n",
    "#     model=base_model,\n",
    "#     args=base_training_args,\n",
    "#     train_dataset=tokenized_dataset['train'],\n",
    "#     eval_dataset=tokenized_dataset['validation'],\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    "# )\n",
    "\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# base_trainer.train()\n",
    "# base_trainer.save_model(\"./base_model\")\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "BASE METRICS:\n",
      " name[Alimentum], area[city centre], familyFriendly[no]  There is a place in the city centre, Alimentum, that is not family-friendly., it is called Alimentum.  It is located in the centre of the city. \n",
      " name[Alimentum], area[city centre], familyFriendly[no]  There is a place in the city centre, Alimentum, that is not family-friendly. \n",
      "decoded Labels: [['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'There', 'is', 'a', 'place', 'in', 'the', 'city', 'centre,', 'Alimentum,', 'that', 'is', 'not', 'family-friendly.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'In', 'the', 'city', 'centre', 'there', 'is', 'a', 'venue', 'name', 'Alimentum,', 'this', 'is', 'not', 'a', 'family-friendly', 'venue.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'Alimentum', 'is', 'not', 'a', 'family-friendly', 'place,', 'located', 'in', 'city', 'centre.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'Alimentum', 'is', 'not', 'a', 'family-friendly', 'arena', 'and', 'is', 'located', 'in', 'the', 'city', 'centre.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'Alimentum', 'is', 'not', 'a', 'family-friendly', 'place', 'in', 'the', 'city', 'centre.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'Alimentum', 'in', 'city', 'centre', 'is', 'not', 'a', 'family-friendly', 'place.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no],', 'near[Burger', 'King]', 'Alimentum', 'is', 'not', 'family-friendly,', 'and', 'is', 'near', 'the', 'Burger', 'King', 'in', 'the', 'city', 'centre.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no],', 'near[Burger', 'King]', 'Near', 'Burger', 'King', 'in', 'city', 'centre', 'is', 'the', 'adult', 'establishment', 'Alimentum.']]\n",
      "Evaluation Metrics: {'bleu': 0.6484381861194152, 'nist': 3.710989293746934, 'meteor': 0.4939702820646838, 'rougeL': 0.7473351255891931, 'cider': 0.6885376731079811}\n"
     ]
    }
   ],
   "source": [
    "# reload model and trainer\n",
    "reloaded_base_model = PeftModel.from_pretrained(base_gpt2_model, \"./base_model\")\n",
    "\n",
    "# reloaded_base_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "base_trainer = Trainer(\n",
    "    model=reloaded_base_model,\n",
    "    args=base_training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "beam_size = 10\n",
    "length_penalty = 0.9\n",
    "no_repeat_ngram_size = 4\n",
    "\n",
    "print('='*20)\n",
    "print(\"BASE METRICS:\")\n",
    "base_metrics = evaluate_with_custom_generation(base_trainer, beam_size=beam_size, length_penalty=length_penalty, no_repeat_ngram_size=no_repeat_ngram_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated before cleaning: 0.48 GB\n",
      "Memory allocated after cleaning: 0.48 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# Check memory allocated on the current device (GPU 0 by default)\n",
    "allocated_memory = torch.cuda.memory_allocated()  # Memory in bytes\n",
    "\n",
    "# Convert bytes to GB\n",
    "allocated_memory_gb = allocated_memory / 1024**3  # Convert bytes to GB\n",
    "\n",
    "print(f\"Memory allocated before cleaning: {allocated_memory_gb:.2f} GB\")\n",
    "\n",
    "del base_gpt2_model\n",
    "del base_model\n",
    "del base_trainer\n",
    "gc.collect()\n",
    "\n",
    "# Check memory allocated on the current device (GPU 0 by default)\n",
    "allocated_memory = torch.cuda.memory_allocated()  # Memory in bytes\n",
    "\n",
    "# Convert bytes to GB\n",
    "allocated_memory_gb = allocated_memory / 1024**3  # Convert bytes to GB\n",
    "\n",
    "print(f\"Memory allocated after cleaning: {allocated_memory_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better LoRA Config and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 3204432\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13145' max='13145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13145/13145 28:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>9.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.096300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>2.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.055900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "BETTER METRICS:\n",
      " name[Alimentum], area[city centre], familyFriendly[no]  There is a place in the city centre, Alimentum, that is not family-friendly.  There is a non-family-friendly place called Alimentum.  Alimentum is located in\n",
      " name[Alimentum], area[city centre], familyFriendly[no]  There is a place in the city centre, Alimentum, that is not family-friendly. \n",
      "decoded Labels: [['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'There', 'is', 'a', 'place', 'in', 'the', 'city', 'centre,', 'Alimentum,', 'that', 'is', 'not', 'family-friendly.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'In', 'the', 'city', 'centre', 'there', 'is', 'a', 'venue', 'name', 'Alimentum,', 'this', 'is', 'not', 'a', 'family-friendly', 'venue.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'Alimentum', 'is', 'not', 'a', 'family-friendly', 'place,', 'located', 'in', 'city', 'centre.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'Alimentum', 'is', 'not', 'a', 'family-friendly', 'arena', 'and', 'is', 'located', 'in', 'the', 'city', 'centre.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'Alimentum', 'is', 'not', 'a', 'family-friendly', 'place', 'in', 'the', 'city', 'centre.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no]', 'Alimentum', 'in', 'city', 'centre', 'is', 'not', 'a', 'family-friendly', 'place.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no],', 'near[Burger', 'King]', 'Alimentum', 'is', 'not', 'family-friendly,', 'and', 'is', 'near', 'the', 'Burger', 'King', 'in', 'the', 'city', 'centre.'], ['name[Alimentum],', 'area[city', 'centre],', 'familyFriendly[no],', 'near[Burger', 'King]', 'Near', 'Burger', 'King', 'in', 'city', 'centre', 'is', 'the', 'adult', 'establishment', 'Alimentum.']]\n",
      "Evaluation Metrics: {'bleu': 0.6812687844889496, 'nist': 3.9610110801465814, 'meteor': 0.4981995233353837, 'rougeL': 0.7465557860693413, 'cider': 0.8528520870310403}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# don't need to deepcopy if deleting in prev cell\n",
    "better_gpt2_model = base_gpt2_model #GPT2LMHeadModel.from_pretrained('gpt2')  # deepcopy(base_gpt2_model)\n",
    "\n",
    "# My implementation to improve upon baseline\n",
    "better_lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\", \"wte\", \"wpe\"],  # only W_q and W_v are used in the benchmark\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # TaskType.LM,\n",
    "    init_lora_weights = \"gaussian\",\n",
    ")\n",
    "better_model = get_peft_model(better_gpt2_model, better_lora_config)\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in better_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "better_training_args = TrainingArguments(\n",
    "    output_dir=\"./better_results\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    label_smoothing_factor=0.1,\n",
    "    logging_dir='./better_logs',\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "better_trainer = Trainer(\n",
    "    model=better_model,\n",
    "    args=better_training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "better_trainer.train()\n",
    "better_trainer.save_model(\"./better_model\")\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Eval\n",
    "beam_size = 10\n",
    "length_penalty = 0.9\n",
    "no_repeat_ngram_size = 4\n",
    "\n",
    "print('='*20)\n",
    "print(\"BETTER METRICS:\")\n",
    "base_metrics = evaluate_with_custom_generation(better_trainer, beam_size=beam_size, length_penalty=length_penalty, no_repeat_ngram_size=no_repeat_ngram_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "567",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
