{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enforce Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 44\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(42)  # if using multi-GPU\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained GPT-2 Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (GPT-2)\n",
    "base_gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add padding token for the tokenizer if not present\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "base_gpt2_model.config.pad_token_id = base_gpt2_model.config.eos_token_id\n",
    "\n",
    "# Add special tokens to the tokenizer\n",
    "special_tokens = {\n",
    "    'bos_token': '<|bos|>',\n",
    "    'eos_token': '<|eos|>',\n",
    "    'pad_token': '<|pad|>',\n",
    "    'sep_token': '<|sep|>'\n",
    "}\n",
    "gpt2_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Initialize model with resized token embeddings\n",
    "base_gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "\n",
    "# Set the pad_token_id in the model configuration\n",
    "base_gpt2_model.config.pad_token_id = gpt2_tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_e2e_for_gpt2(tokenizer):\n",
    "    dataset = load_dataset(\"e2e_nlg\")\n",
    "    # print(dataset[\"test\"][\"meaning_representation\"])\n",
    "\n",
    "    # # save test set for metric computation with official script\n",
    "    # testSet = dataset[\"test\"]\n",
    "    # df = testSet.to_pandas()\n",
    "    # df.to_csv(\"e2e_nlg_test_set.csv\", index=False)\n",
    "\n",
    "    # Add special tokens\n",
    "    special_tokens = {\n",
    "        'bos_token': '<|bos|>',\n",
    "        'eos_token': '<|eos|>',\n",
    "        'pad_token': '<|pad|>',\n",
    "        'sep_token': '<|sep|>'\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    def preprocess_train(examples):\n",
    "        # Format: <|bos|> meaning_representation <|sep|> target <|eos|>\n",
    "        texts = [\n",
    "            f\"{tokenizer.bos_token} {mr} {tokenizer.sep_token} {ref} {tokenizer.eos_token}\"\n",
    "            for mr, ref in zip(examples['meaning_representation'], examples['human_reference'])\n",
    "        ]\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "        )\n",
    "        \n",
    "        # Create labels (same as input_ids initially)\n",
    "        labels = encodings['input_ids']\n",
    "    \n",
    "        # Mask out MR tokens in labels\n",
    "        for i, (mr, ref) in enumerate(zip(examples['meaning_representation'], examples['human_reference'])):\n",
    "            # Tokenize MR and find its length\n",
    "            mr_tokens = tokenizer(f\"{tokenizer.bos_token} {mr} {tokenizer.sep_token}\", add_special_tokens=False).input_ids\n",
    "        \n",
    "            # Mask MR tokens with -100\n",
    "            num_mr_tokens = len(mr_tokens)\n",
    "            labels[i][:num_mr_tokens] = [-100] * num_mr_tokens\n",
    "    \n",
    "        # Add labels to the encodings\n",
    "        encodings['labels'] = labels\n",
    "        return encodings\n",
    "\n",
    "        # # Create labels (same as input_ids)\n",
    "        # encodings['labels'] = encodings['input_ids']\n",
    "        # return encodings\n",
    "    \n",
    "\n",
    "    def preprocess_test(examples):\n",
    "        mr_texts = [\n",
    "            f\"{tokenizer.bos_token} {mr} {tokenizer.sep_token} \"\n",
    "            for mr in examples['meaning_representation']\n",
    "        ]\n",
    "\n",
    "        mr_encodings = tokenizer(\n",
    "            mr_texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=64,\n",
    "        )\n",
    "    \n",
    "        ref_texts = [\n",
    "            f\"{ref} {tokenizer.eos_token}\"\n",
    "            for ref in examples['human_reference']\n",
    "        ]\n",
    "\n",
    "        ref_encodings = tokenizer(\n",
    "            ref_texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=64,\n",
    "        )\n",
    "\n",
    "        mr_encodings['labels'] = ref_encodings['input_ids']\n",
    "        return mr_encodings\n",
    "\n",
    "\n",
    "    # Process all splits\n",
    "    tokenized_train = dataset.map(\n",
    "        preprocess_train,\n",
    "        batched=True,\n",
    "        remove_columns=dataset['train'].column_names\n",
    "    )\n",
    "\n",
    "    tokenized_test = dataset.map(\n",
    "        preprocess_test,\n",
    "        batched=True,\n",
    "    )\n",
    "\n",
    "    # Set format to PyTorch tensors\n",
    "    tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    return tokenized_train, tokenized_test\n",
    "#=====================================================\n",
    "\n",
    "tokenized_train, tokenized_test = prep_e2e_for_gpt2(gpt2_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from nltk.translate.nist_score import sentence_nist\n",
    "\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "cider = Cider()\n",
    "meteor = Meteor()\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions = pred[\"predictions\"]\n",
    "    labels = pred[\"label_ids\"]\n",
    "    \n",
    "    print(predictions[0])\n",
    "    # print(labels[0])\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = predictions #gpt2_tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = labels #gpt2_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # decoded_labels = [[label] for label in decoded_labels]  # Format for some metrics\n",
    "\n",
    "    # Compute BLEU\n",
    "    bleu_score = bleu.compute(predictions=decoded_preds, references=decoded_labels)[\"bleu\"]\n",
    "    \n",
    "    # Compute ROUGE-L\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels, rouge_types=[\"rougeL\"])\n",
    "    rougeL_score = rouge_result[\"rougeL\"]\n",
    "    \n",
    "    # Compute CIDEr\n",
    "    # Format the predictions and references into dictionaries with index as key\n",
    "    # CIDEr expects the input format to be a dictionary with example ID as the key\n",
    "    refs = {i: [label] for i, label in enumerate(decoded_labels)}\n",
    "    preds = {i: [pred] for i, pred in enumerate(decoded_preds)}\n",
    "    cider_score, _ = cider.compute_score(refs, preds)\n",
    "    \n",
    "    # Compute METEOR\n",
    "    meteor_score, _ = meteor.compute_score(preds, refs)\n",
    "    \n",
    "    # Compute NIST\n",
    "    nist_scores = []\n",
    "    decoded_labels = [label.split() for label in decoded_labels]\n",
    "    # print(f\"decoded Labels: {decoded_labels}\")\n",
    "    for candidate in decoded_preds:\n",
    "        nist_score = sentence_nist(decoded_labels, candidate.split())  # You can adjust n as needed\n",
    "        nist_scores.append(nist_score)\n",
    "\n",
    "    avg_nist_score = sum(nist_scores) / len(nist_scores) if nist_scores else 0  # Average NIST score\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"nist\": avg_nist_score,\n",
    "        \"meteor\": meteor_score,\n",
    "        \"rougeL\": rougeL_score,\n",
    "        \"cider\": cider_score,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom Eval/Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def evaluate_with_custom_generation(trainer, out_filename, beam_size, length_penalty, no_repeat_ngram_size, batch_size=8):\n",
    "    model = trainer.model\n",
    "    tokenizer = gpt2_tokenizer\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # DataLoader for evaluation dataset\n",
    "    eval_dataloader = DataLoader(\n",
    "        trainer.eval_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    eval_predictions = []\n",
    "    eval_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "\n",
    "            # print(f\"batch: {tokenizer.batch_decode(input_ids, skip_special_tokens=True)}\")\n",
    "            # print(f\"input ids: {input_ids}\")\n",
    "            \n",
    "            # Generate predictions with beam search\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=beam_size,\n",
    "                length_penalty=length_penalty,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                max_length=input_ids.size(1) + 40,\n",
    "                eos_token_id=model.config.eos_token_id,  # Ensure eos_token_id is set\n",
    "                early_stopping=True,\n",
    "            )\n",
    "\n",
    "            # print(model.config.eos_token_id)\n",
    "            # Remove the input_ids from the generated outputs\n",
    "            # Slice the generated output to remove input part (this assumes `outputs` includes the input tokens)\n",
    "            generated_ids = outputs[:, input_ids.size(1):]  # Skip the input length\n",
    "\n",
    "            # Decode predictions\n",
    "            decoded_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            eval_predictions.extend(decoded_outputs)\n",
    "\n",
    "            # Decode labels for metric comparison\n",
    "            decoded_labels = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "            eval_labels.extend(decoded_labels)\n",
    "            break  #=====================================================================\n",
    "\n",
    "    # Prepare predictions and labels for metric computation\n",
    "    pred = {\n",
    "        'predictions': eval_predictions,\n",
    "        'label_ids': eval_labels\n",
    "    }\n",
    "\n",
    "    # save preds to file\n",
    "    with open(out_filename, 'w') as f:\n",
    "        f.writelines(item + \"\\n\" for item in eval_predictions)\n",
    "\n",
    "    # Compute custom evaluation metrics\n",
    "    custom_eval_metrics = trainer.compute_metrics(pred)\n",
    "    print(\"Evaluation Metrics:\", custom_eval_metrics)\n",
    "\n",
    "    return custom_eval_metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LoRA Config and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 147456\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# From the original implementation in https://arxiv.org/abs/2106.09685\n",
    "# Note that we are using vanilla GPT-2 (i.e. GPT-2 Small), rather than the original GPT-2 Medium\n",
    "base_lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\"],  # only W_q and W_v are used in the benchmark\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # TaskType.LM,\n",
    "    init_lora_weights = True,\n",
    ")\n",
    "base_model = get_peft_model(base_gpt2_model, base_lora_config)\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "# all batches originally 8, but made smaller bc ran out of memory\n",
    "base_training_args = TrainingArguments(\n",
    "    output_dir=\"./base_results\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    label_smoothing_factor=0.1,\n",
    "    logging_dir='./base_logs',\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "base_trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=base_training_args,\n",
    "    train_dataset=tokenized_train['train'],\n",
    "    eval_dataset=tokenized_test['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "\n",
    "base_trainer.train()\n",
    "base_trainer.save_model(\"./base_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "BASE METRICS:\n",
      "batch: [' name[Blue Spice], eatType[coffee shop], area[city centre]  ', ' name[Blue Spice], eatType[coffee shop], area[city centre]  ', ' name[Blue Spice], eatType[coffee shop], area[riverside]  ', ' name[Blue Spice], eatType[coffee shop], area[riverside]  ', ' name[Blue Spice], eatType[coffee shop], customer rating[5 out of 5], near[Crowne Plaza Hotel]  ', ' name[Blue Spice], eatType[coffee shop], customer rating[5 out of 5], near[Crowne Plaza Hotel]  ', ' name[Blue Spice], eatType[coffee shop], customer rating[5 out of 5], near[Crowne Plaza Hotel]  ', ' name[Blue Spice], eatType[coffee shop], customer rating[average], near[Burger King]  ']\n",
      "                   � ................  \n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBASE METRICS:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m base_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_with_custom_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase-output.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 64\u001b[0m, in \u001b[0;36mevaluate_with_custom_generation\u001b[0;34m(trainer, out_filename, beam_size, length_penalty, no_repeat_ngram_size, batch_size)\u001b[0m\n\u001b[1;32m     61\u001b[0m     f\u001b[38;5;241m.\u001b[39mwritelines(item \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m eval_predictions)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Compute custom evaluation metrics\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m custom_eval_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m, custom_eval_metrics)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m custom_eval_metrics\n",
      "Cell \u001b[0;32mIn[9], line 46\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print(f\"decoded Labels: {decoded_labels}\")\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m decoded_preds:\n\u001b[0;32m---> 46\u001b[0m     nist_score \u001b[38;5;241m=\u001b[39m \u001b[43msentence_nist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# You can adjust n as needed\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     nist_scores\u001b[38;5;241m.\u001b[39mappend(nist_score)\n\u001b[1;32m     49\u001b[0m avg_nist_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(nist_scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(nist_scores) \u001b[38;5;28;01mif\u001b[39;00m nist_scores \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Average NIST score\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/nltk/translate/nist_score.py:70\u001b[0m, in \u001b[0;36msentence_nist\u001b[0;34m(references, hypothesis, n)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_nist\u001b[39m(references, hypothesis, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Calculate NIST score from\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    George Doddington. 2002. \"Automatic evaluation of machine translation quality\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    :type n: int\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorpus_nist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/567/lib/python3.11/site-packages/nltk/translate/nist_score.py:165\u001b[0m, in \u001b[0;36mcorpus_nist\u001b[0;34m(list_of_references, hypotheses, n)\u001b[0m\n\u001b[1;32m    162\u001b[0m nist_precision \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nist_precision_numerator_per_ngram:\n\u001b[1;32m    164\u001b[0m     precision \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 165\u001b[0m         \u001b[43mnist_precision_numerator_per_ngram\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnist_precision_denominator_per_ngram\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m     nist_precision \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m precision\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Eqn 3 in Doddington(2002)\u001b[39;00m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# reload model and trainer\n",
    "reloaded_base_model = PeftModel.from_pretrained(base_gpt2_model, \"./base_model\")\n",
    "\n",
    "# reloaded_base_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "base_trainer = Trainer(\n",
    "    model=reloaded_base_model,\n",
    "    args=base_training_args,\n",
    "    train_dataset=tokenized_train['train'],\n",
    "    eval_dataset=tokenized_test['test'],  #tokenized_train['validation'],  #['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "beam_size = 10\n",
    "length_penalty = 0.9\n",
    "no_repeat_ngram_size = 4\n",
    "\n",
    "print('='*20)\n",
    "print(\"BASE METRICS:\")\n",
    "base_metrics = evaluate_with_custom_generation(base_trainer, out_filename=\"base-output.txt\", beam_size=beam_size, length_penalty=length_penalty, no_repeat_ngram_size=no_repeat_ngram_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better LoRA Config and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 3204432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanswif/miniconda3/envs/567/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "better_gpt2_model = deepcopy(base_gpt2_model)\n",
    "\n",
    "# My implementation to improve upon baseline\n",
    "better_lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\", \"wte\", \"wpe\"],  # only W_q and W_v are used in the benchmark\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # TaskType.LM,\n",
    "    init_lora_weights = \"gaussian\",\n",
    ")\n",
    "better_model = get_peft_model(better_gpt2_model, better_lora_config)\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in better_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "better_training_args = TrainingArguments(\n",
    "    output_dir=\"./better_results\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    label_smoothing_factor=0.1,\n",
    "    logging_dir='./better_logs',\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# better_trainer = Trainer(\n",
    "#     model=better_model,\n",
    "#     args=better_training_args,\n",
    "#     train_dataset=tokenized_train['train'],\n",
    "#     eval_dataset=tokenized_test['test'],\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    "# )\n",
    "\n",
    "\n",
    "# better_trainer.train()\n",
    "# better_trainer.save_model(\"./better_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Better Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "BETTER METRICS:\n",
      "Evaluation Metrics: {'bleu': 0.20487219861156433, 'nist': 2.5084701959526274, 'meteor': 0.23775311842805094, 'rougeL': 0.46319725238068893, 'cider': 0.12804611983750253}\n"
     ]
    }
   ],
   "source": [
    "# reload model and trainer\n",
    "reloaded_better_model = PeftModel.from_pretrained(better_gpt2_model, \"./better_model\")\n",
    "\n",
    "# reloaded_base_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "better_trainer = Trainer(\n",
    "    model=reloaded_better_model,\n",
    "    args=better_training_args,\n",
    "    train_dataset=tokenized_train['train'],\n",
    "    eval_dataset=tokenized_test['test'],  #['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=gpt2_tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "beam_size = 10\n",
    "length_penalty = 0.9\n",
    "no_repeat_ngram_size = 4\n",
    "\n",
    "print('='*20)\n",
    "print(\"BETTER METRICS:\")\n",
    "better_metrics = evaluate_with_custom_generation(better_trainer, out_filename=\"better-output.txt\", beam_size=beam_size, length_penalty=length_penalty, no_repeat_ngram_size=no_repeat_ngram_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "567",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
